{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c092a3b",
   "metadata": {},
   "source": [
    "In this tutorial, we'll demonstrate the Argoverse 2.0 map API, and visualize some of the map data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f25e1e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15a8196d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/scene-rep/u/gcaz/lidar_compressive_sensing/av2-api/src/av2/rendering/ops/draw.py:15: RuntimeWarning: overflow encountered in scalar add\n",
      "  UINT8_BITS: Final[np.uint8] = np.log2(UINT8_MAX + 1).astype(np.uint8)\n",
      "/data/scene-rep/u/gcaz/lidar_compressive_sensing/av2-api/src/av2/rendering/ops/draw.py:15: RuntimeWarning: divide by zero encountered in log2\n",
      "  UINT8_BITS: Final[np.uint8] = np.log2(UINT8_MAX + 1).astype(np.uint8)\n",
      "/data/scene-rep/u/gcaz/lidar_compressive_sensing/av2-api/src/av2/rendering/ops/draw.py:15: RuntimeWarning: invalid value encountered in cast\n",
      "  UINT8_BITS: Final[np.uint8] = np.log2(UINT8_MAX + 1).astype(np.uint8)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import av2.geometry.polyline_utils as polyline_utils\n",
    "import av2.rendering.vector as vector_plotting_utils\n",
    "from av2.datasets.sensor.av2_sensor_dataloader import AV2SensorDataLoader\n",
    "from av2.map.map_api import ArgoverseStaticMap, LaneSegment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d1aaf7",
   "metadata": {},
   "source": [
    "First, we'll plot the lane graph and crosswalks, with crosswalks colored purple, and the lanes colored green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d605e8-7f46-44dd-ba59-eb1107653d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Final\n",
    "\n",
    "import click\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import av2.rendering.color as color_utils\n",
    "import av2.rendering.rasterize as raster_rendering_utils\n",
    "import av2.rendering.video as video_utils\n",
    "import av2.utils.io as io_utils\n",
    "import av2.utils.raster as raster_utils\n",
    "from av2.datasets.sensor.av2_sensor_dataloader import AV2SensorDataLoader\n",
    "from av2.datasets.sensor.constants import RingCameras\n",
    "from av2.map.map_api import ArgoverseStaticMap\n",
    "from av2.rendering.color import GREEN_HEX, RED_HEX\n",
    "from av2.utils.typing import NDArrayByte, NDArrayFloat, NDArrayInt\n",
    "import av2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b3cc09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_egoview_overlaid_lidar(\n",
    "    data_root: Path,\n",
    "    output_dir: Path,\n",
    "    log_id: str,\n",
    "    render_ground_pts_only: bool,\n",
    "    dump_single_frames: bool,\n",
    ") -> None:\n",
    "    \"\"\"Render LiDAR points from a particular camera's viewpoint (color by ground surface, and apply ROI filtering).\n",
    "\n",
    "    Args:\n",
    "        data_root: path to directory where the logs live on disk.\n",
    "        output_dir: path to directory where renderings will be saved.\n",
    "        log_id: unique ID for AV2 scenario/log.\n",
    "        render_ground_pts_only: whether to only render LiDAR points located close to the ground surface.\n",
    "        dump_single_frames: Whether to save to disk individual RGB frames of the rendering, in addition to generating\n",
    "            the mp4 file.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If vehicle log data is not present at `data_root` for `log_id`.\n",
    "    \"\"\"\n",
    "    loader = AV2SensorDataLoader(data_dir=data_root, labels_dir=data_root)\n",
    "\n",
    "    log_map_dirpath = data_root / log_id / \"map\"\n",
    "    avm = ArgoverseStaticMap.from_map_dir(log_map_dirpath, build_raster=True)\n",
    "    # print(avm)\n",
    "\n",
    "    # repeat red to green colormap every 50 m.\n",
    "    colors_arr_rgb = color_utils.create_colormap(\n",
    "        color_list=[RED_HEX, GREEN_HEX], n_colors=NUM_RANGE_BINS\n",
    "    )\n",
    "    colors_arr_rgb = (colors_arr_rgb * 255).astype(np.uint8)\n",
    "    colors_arr_bgr: NDArrayByte = np.fliplr(colors_arr_rgb)\n",
    "\n",
    "    print(list(RingCameras))\n",
    "    for _, cam_name in enumerate(list(RingCameras)):\n",
    "        print(type(cam_name))\n",
    "        cam_im_fpaths = loader.get_ordered_log_cam_fpaths(log_id, cam_name.value)\n",
    "        num_cam_imgs = len(cam_im_fpaths)\n",
    "\n",
    "        video_list = []\n",
    "        # print(cam_im_fpaths)\n",
    "        for i, im_fpath in enumerate(cam_im_fpaths):\n",
    "            if i % 50 == 0:\n",
    "                logging.info(\n",
    "                    f\"\\tOn file {i}/{num_cam_imgs} of camera {cam_name} of {log_id}\"\n",
    "                )\n",
    "\n",
    "            cam_timestamp_ns = int(im_fpath.stem)\n",
    "            city_SE3_ego = loader.get_city_SE3_ego(log_id, cam_timestamp_ns)\n",
    "            \n",
    "            if city_SE3_ego is None:\n",
    "                logger.exception(\"missing LiDAR pose\")\n",
    "                continue\n",
    "\n",
    "            # load feather file path, e.g. '315978406032859416.feather\"\n",
    "            lidar_fpath = loader.get_closest_lidar_fpath((data_root / log_id).name, cam_timestamp_ns)\n",
    "            # print(lidar_fpath)\n",
    "            # print(cam_timestamp_ns)\n",
    "            # print(loader._sdb.get_closest_lidar_timestamp(cam_timestamp_ns, (data_root / log_id).name))\n",
    "            if lidar_fpath is None:\n",
    "                # logger.info(\n",
    "                #     \"No LiDAR sweep found within the synchronization interval for %s, so skipping...\",\n",
    "                #     cam_name,\n",
    "                # )\n",
    "                continue\n",
    "\n",
    "            img_bgr = io_utils.read_img(im_fpath, channel_order=\"BGR\")\n",
    "\n",
    "            lidar_points_ego = io_utils.read_lidar_sweep(lidar_fpath, attrib_spec=\"xyz\")\n",
    "            lidar_timestamp_ns = int(lidar_fpath.stem)\n",
    "\n",
    "            # put into city coords, then prune away ground and non-RoI points\n",
    "            lidar_points_city = city_SE3_ego.transform_point_cloud(lidar_points_ego)\n",
    "            lidar_points_city = avm.remove_non_drivable_area_points(lidar_points_city)\n",
    "            is_ground_logicals = avm.get_ground_points_boolean(lidar_points_city)\n",
    "            # lidar_points_city = lidar_points_city[\n",
    "            #     is_ground_logicals if render_ground_pts_only else ~is_ground_logicals\n",
    "            # ]\n",
    "            lidar_points_ego = city_SE3_ego.inverse().transform_point_cloud(\n",
    "                lidar_points_city\n",
    "            )\n",
    "\n",
    "            # motion compensate always\n",
    "            (\n",
    "                uv,\n",
    "                points_cam,\n",
    "                is_valid_points,\n",
    "            ) = loader.project_ego_to_img_motion_compensated(\n",
    "                points_lidar_time=lidar_points_ego,\n",
    "                cam_name=cam_name,\n",
    "                cam_timestamp_ns=cam_timestamp_ns,\n",
    "                lidar_timestamp_ns=lidar_timestamp_ns,\n",
    "                log_id=log_id,\n",
    "            )\n",
    "\n",
    "            if is_valid_points is None or uv is None or points_cam is None:\n",
    "                continue\n",
    "\n",
    "            if is_valid_points.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            uv_int: NDArrayInt = np.round(uv[is_valid_points]).astype(np.int32)\n",
    "            points_cam = points_cam[is_valid_points]\n",
    "            pt_ranges: NDArrayFloat = np.linalg.norm(points_cam[:, :3], axis=1)\n",
    "            color_bins: NDArrayInt = np.round(pt_ranges).astype(np.int32)\n",
    "            # account for moving past 100 meters, loop around again\n",
    "            color_bins = color_bins % NUM_RANGE_BINS\n",
    "            uv_colors_bgr = colors_arr_bgr[color_bins]\n",
    "\n",
    "            img_empty = np.full_like(img_bgr, fill_value=0)\n",
    "            # print(uv_int)\n",
    "            # print(uv_colors_bgr)\n",
    "            # print(img_empty.shape, uv_int.shape, uv_colors_bgr.shape)\n",
    "            # print(img_empty[uv_int])\n",
    "            # img_empty[uv_int] = uv_colors_bgr\n",
    "            # print(img_empty)\n",
    "            # img_empty = raster_rendering_utils.draw_points_xy_in_img(\n",
    "            #     img_empty, uv_int, uv_colors_bgr, diameter=5, with_anti_alias=False, sigma=1.0, alpha=0.0\n",
    "            # )\n",
    "            # blended_bgr = raster_utils.blend_images(img_bgr, img_empty)\n",
    "            # frame_rgb = blended_bgr[:, :, ::-1]\n",
    "\n",
    "            blended_bgr = raster_rendering_utils.draw_points_xy_in_img(\n",
    "                img_bgr, uv_int, uv_colors_bgr, diameter=5, with_anti_alias=False, sigma=1.0, alpha=0.0\n",
    "            )\n",
    "            # blended_bgr = raster_utils.blend_images(img_bgr, img_empty)\n",
    "            frame_rgb = blended_bgr[:, :, ::-1]\n",
    "\n",
    "            if dump_single_frames:\n",
    "                save_dir = output_dir / log_id / cam_name\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                cv2.imwrite(\n",
    "                    str(save_dir / f\"{cam_name}_{lidar_timestamp_ns}.jpg\"), blended_bgr\n",
    "                )\n",
    "\n",
    "            video_list.append(frame_rgb)\n",
    "\n",
    "        if len(video_list) == 0:\n",
    "            raise RuntimeError(\n",
    "                \"No video frames were found; log data was not found on disk.\"\n",
    "            )\n",
    "\n",
    "        video: NDArrayByte = np.stack(video_list).astype(np.uint8)\n",
    "        video_output_dir = Path(output_dir) / Path(\"videos\")\n",
    "        video_utils.write_video(\n",
    "            video=video,\n",
    "            dst=video_output_dir / f\"{log_id}_{cam_name}.mp4\",\n",
    "            fps=RING_CAMERA_FPS,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23119ce6-5a46-4730-8ce7-4926faffd800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<RingCameras.RING_REAR_LEFT: 'ring_rear_left'>, <RingCameras.RING_SIDE_LEFT: 'ring_side_left'>, <RingCameras.RING_FRONT_LEFT: 'ring_front_left'>, <RingCameras.RING_FRONT_CENTER: 'ring_front_center'>, <RingCameras.RING_FRONT_RIGHT: 'ring_front_right'>, <RingCameras.RING_SIDE_RIGHT: 'ring_side_right'>, <RingCameras.RING_REAR_RIGHT: 'ring_rear_right'>]\n",
      "<enum 'RingCameras'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No video frames were found; log data was not found on disk.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m NUM_RANGE_BINS: Final[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m      8\u001b[0m RING_CAMERA_FPS: Final[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mgenerate_egoview_overlaid_lidar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./av2_out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrender_ground_pts_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdump_single_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 139\u001b[0m, in \u001b[0;36mgenerate_egoview_overlaid_lidar\u001b[0;34m(data_root, output_dir, log_id, render_ground_pts_only, dump_single_frames)\u001b[0m\n\u001b[1;32m    136\u001b[0m     video_list\u001b[38;5;241m.\u001b[39mappend(frame_rgb)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(video_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo video frames were found; log data was not found on disk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    143\u001b[0m video: NDArrayByte \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(video_list)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m    144\u001b[0m video_output_dir \u001b[38;5;241m=\u001b[39m Path(output_dir) \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideos\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No video frames were found; log data was not found on disk."
     ]
    }
   ],
   "source": [
    "# path to where the logs live\n",
    "dataroot = Path(\"av2-api/tests/unit/test_data/sensor/test\")\n",
    "\n",
    "# unique log identifier\n",
    "log_id = Path(\"7fab2350-7eaf-3b7e-a39d-6937a4c1bede\")\n",
    "\n",
    "NUM_RANGE_BINS: Final[int] = 50\n",
    "RING_CAMERA_FPS: Final[int] = 20\n",
    "\n",
    "generate_egoview_overlaid_lidar(\n",
    "        data_root=dataroot,\n",
    "        output_dir=\"./av2_out\",\n",
    "        log_id=log_id,\n",
    "        render_ground_pts_only=False,\n",
    "        dump_single_frames=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5448f9bb-8165-486f-ba9c-d4b2254aa16c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
